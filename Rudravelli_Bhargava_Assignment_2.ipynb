{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavaCharyRudravelli/Bhargava_INFO5731_Fall2024/blob/main/Rudravelli_Bhargava_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSTxAA5M9ueK",
        "outputId": "8dfb39e7-02d9-4e8a-f01a-2b5e4f874c53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USNeMoen01Ce",
        "outputId": "01333e38-d981-43e7-87c6-1fac8faec22d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Get connection from the browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "# Function to scrape reviews from the HTML content\n",
        "def scrape_reviews(soup):\n",
        "    review_containers = soup.find_all(class_='review-container')\n",
        "    reviews = []\n",
        "    for item in review_containers:\n",
        "        username = item.find(class_='display-name-link').a.get_text() if item.find(class_='display-name-link') else \"N/A\"\n",
        "        review = item.find(class_='text').get_text() if item.find(class_='text') else \"N/A\"\n",
        "        rating_element = item.find(class_='rating-other-user-rating')\n",
        "        rating = rating_element.get_text().strip() if rating_element else \"N/A\"\n",
        "        reviews.append({\n",
        "            'username': username,\n",
        "            'review': review,\n",
        "            'rating': rating\n",
        "        })\n",
        "    return reviews\n",
        "\n",
        "# this function is used to load more reviews from the page\n",
        "def get_more_reviews(pagination_key):\n",
        "    ajax_url = f\"https://www.imdb.com/title/tt0499549/reviews/_ajax?paginationKey={pagination_key}\"\n",
        "    response = requests.get(ajax_url, headers=headers)\n",
        "    return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Main function to scrape IMDb reviews, including dynamic loading of more reviews\n",
        "def scrape_imdb_reviews(movie_url, number_of_reviews=1000):\n",
        "    all_reviews = []\n",
        "    # Initial request to get the first set of reviews\n",
        "    response = requests.get(movie_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Scrape reviews from the initial page\n",
        "    all_reviews.extend(scrape_reviews(soup))\n",
        "    # Debug: Print how many reviews were scraped initially\n",
        "    print(f\"Initial reviews scraped: {len(all_reviews)}\")\n",
        "    # Loop to load more reviews using the AJAX pagination\n",
        "    while len(all_reviews) < number_of_reviews:\n",
        "        # Find the \"Load More\" button and check if it exists\n",
        "        load_more_button = soup.find('div', class_='load-more-data')\n",
        "        data_key = soup.find('div', class_='load-more-data') #this is needed to load more reviews from html\n",
        "        if load_more_button:\n",
        "            # Check if the button has the 'data-key' attribute before accessing it\n",
        "            if 'data-key' in load_more_button.attrs:\n",
        "                pagination_key = load_more_button['data-key']\n",
        "                print(\"Pagination key found: {pagination_key}\")\n",
        "\n",
        "                # Get more reviews using AJAX\n",
        "                soup = get_more_reviews(pagination_key)\n",
        "                new_reviews = scrape_reviews(soup)\n",
        "                print(\"more reviews scraped: {len(new_reviews)}\")\n",
        "                all_reviews.extend(new_reviews)\n",
        "            else:\n",
        "                print(\"No 'data-key' attribute found on the Load More button.\")\n",
        "                break\n",
        "        else:\n",
        "            # No more \"Load More\" button means no more reviews to load\n",
        "            print(\"No 'Load More' button found.\")\n",
        "            break\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return all_reviews[:number_of_reviews]\n",
        "\n",
        "# Scraping 1000 reviews from IMDb for the movie\n",
        "movie_url = \"https://www.imdb.com/title/tt0499549/reviews/?ref_=tt_ov_ql_2\"\n",
        "reviews_data = scrape_imdb_reviews(movie_url, number_of_reviews=1000)\n",
        "\n",
        "# Check if reviews are being scraped\n",
        "print(f\"Total reviews scraped: {len(reviews_data)}\")\n",
        "\n",
        "# Save to CSV file if reviews are available\n",
        "if reviews_data:\n",
        "    with open('imdb_reviews.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['username', 'review', 'rating'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(reviews_data)\n",
        "    print(\"CSV file 'imdb_reviews.csv' created successfully.\")\n",
        "else:\n",
        "    print(\"No reviews were scraped.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR-tPXGL3ppD",
        "outputId": "e23ed96c-b8c5-4611-db5d-fd69b08909a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial reviews scraped: 25\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Pagination key found: {pagination_key}\n",
            "more reviews scraped: {len(new_reviews)}\n",
            "Total reviews scraped: 1000\n",
            "CSV file 'imdb_reviews.csv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcbb89e-367a-490e-b617-4eba49f7a103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'cleaned_imdb_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load the CSV file with original reviews\n",
        "df = pd.read_csv('imdb_reviews.csv')\n",
        "\n",
        "# Initializing necessary components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean each review\n",
        "def clean_text(text):\n",
        "    # Step 1: Remove special characters and punctuations\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes special characters\n",
        "\n",
        "    # Step 2: Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)  # Removes digits\n",
        "\n",
        "    # Step 3: Tokenize the text and remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Step 4: Lowercase all texts\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # Step 5: Stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Step 6: Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "    # Return the cleaned text\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply the clean_text function to the 'review' column\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Saving the dataframe with the cleaned reviews into a new CSV file\n",
        "df.to_csv('cleaned_imdb_reviews.csv', index=False)\n",
        "print(\"Cleaned data saved in 'cleaned_imdb_reviews.csv'.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c84eec-5678-4be2-fbaa-c81b869bd188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Counts:\n",
            "Nouns (N): 49848\n",
            "Verbs (V): 23616\n",
            "Adjectives (Adj): 19033\n",
            "Adverbs (Adv): 5960\n",
            "Sentence: rather intens privileg view jame cameron much anticip million budget return direct scene avatar empir leicest squar londonwher begin visual piec groundbreak termin seri titan one would expect cameron deliv visual far sharpest cgi seen could almost say disquiet follow cameron soul possibl strong intensifi qualiti product design visual effect noteworthi get prais upon offici releasewhat lack realli shape movi characterstori expect complex believ plot left movi mostli strong visual scifi lover desir mindbend philosophi fantasi explor limit outer speci factor would give voteavatar success cameron legaci intellig viral market avatar power market techniqu assembl success blockbust blair witch project rememb dark knight joker invad world also current product artific theartificecom intellig target marketkudo cameron avatar one movi year could get troubl share guy earli plea click ye comment use thank\n",
            "Dependency Parsing:\n",
            "rather (advmod) <-- intens\n",
            "intens (nsubj) <-- begin\n",
            "privileg (compound) <-- view\n",
            "view (compound) <-- cameron\n",
            "jame (compound) <-- cameron\n",
            "cameron (dobj) <-- intens\n",
            "much (amod) <-- million\n",
            "anticip (compound) <-- million\n",
            "million (nummod) <-- budget\n",
            "budget (compound) <-- return\n",
            "return (nmod) <-- londonwher\n",
            "direct (amod) <-- scene\n",
            "scene (compound) <-- empir\n",
            "avatar (compound) <-- empir\n",
            "empir (compound) <-- leicest\n",
            "leicest (compound) <-- squar\n",
            "squar (compound) <-- londonwher\n",
            "londonwher (nsubj) <-- begin\n",
            "begin (ROOT) <-- begin\n",
            "visual (amod) <-- piec\n",
            "piec (compound) <-- titan\n",
            "groundbreak (compound) <-- termin\n",
            "termin (compound) <-- titan\n",
            "seri (compound) <-- titan\n",
            "titan (dobj) <-- begin\n",
            "one (nsubj) <-- expect\n",
            "would (aux) <-- expect\n",
            "expect (ccomp) <-- begin\n",
            "cameron (compound) <-- deliv\n",
            "deliv (nsubj) <-- say\n",
            "visual (amod) <-- cgi\n",
            "far (advmod) <-- sharpest\n",
            "sharpest (amod) <-- cgi\n",
            "cgi (nsubj) <-- say\n",
            "seen (acl) <-- cgi\n",
            "could (aux) <-- say\n",
            "almost (advmod) <-- say\n",
            "say (ccomp) <-- expect\n",
            "disquiet (compound) <-- follow\n",
            "follow (compound) <-- cameron\n",
            "cameron (compound) <-- soul\n",
            "soul (compound) <-- possibl\n",
            "possibl (compound) <-- noteworthi\n",
            "strong (amod) <-- intensifi\n",
            "intensifi (compound) <-- noteworthi\n",
            "qualiti (compound) <-- design\n",
            "product (compound) <-- design\n",
            "design (compound) <-- noteworthi\n",
            "visual (amod) <-- effect\n",
            "effect (compound) <-- noteworthi\n",
            "noteworthi (nsubj) <-- get\n",
            "get (ccomp) <-- say\n",
            "prais (dobj) <-- get\n",
            "upon (prep) <-- get\n",
            "offici (compound) <-- releasewhat\n",
            "releasewhat (compound) <-- characterstori\n",
            "lack (compound) <-- realli\n",
            "realli (compound) <-- shape\n",
            "shape (compound) <-- movi\n",
            "movi (compound) <-- characterstori\n",
            "characterstori (pobj) <-- upon\n",
            "expect (dep) <-- begin\n",
            "complex (amod) <-- plot\n",
            "believ (compound) <-- plot\n",
            "plot (nsubj) <-- left\n",
            "left (ccomp) <-- expect\n",
            "movi (compound) <-- mostli\n",
            "mostli (amod) <-- desir\n",
            "strong (amod) <-- desir\n",
            "visual (amod) <-- scifi\n",
            "scifi (compound) <-- desir\n",
            "lover (compound) <-- desir\n",
            "desir (nsubj) <-- mindbend\n",
            "mindbend (ccomp) <-- left\n",
            "philosophi (compound) <-- fantasi\n",
            "fantasi (compound) <-- factor\n",
            "explor (compound) <-- limit\n",
            "limit (compound) <-- factor\n",
            "outer (amod) <-- factor\n",
            "speci (compound) <-- factor\n",
            "factor (nsubj) <-- give\n",
            "would (aux) <-- give\n",
            "give (ccomp) <-- mindbend\n",
            "voteavatar (compound) <-- success\n",
            "success (compound) <-- intellig\n",
            "cameron (compound) <-- intellig\n",
            "legaci (compound) <-- intellig\n",
            "intellig (nmod) <-- project\n",
            "viral (amod) <-- market\n",
            "market (nmod) <-- market\n",
            "avatar (compound) <-- market\n",
            "power (compound) <-- market\n",
            "market (compound) <-- techniqu\n",
            "techniqu (compound) <-- success\n",
            "assembl (compound) <-- success\n",
            "success (compound) <-- project\n",
            "blockbust (compound) <-- project\n",
            "blair (compound) <-- project\n",
            "witch (compound) <-- project\n",
            "project (nmod) <-- joker\n",
            "rememb (amod) <-- dark\n",
            "dark (amod) <-- joker\n",
            "knight (compound) <-- joker\n",
            "joker (dobj) <-- give\n",
            "invad (compound) <-- world\n",
            "world (dative) <-- give\n",
            "also (advmod) <-- current\n",
            "current (amod) <-- artific\n",
            "product (compound) <-- artific\n",
            "artific (nmod) <-- year\n",
            "theartificecom (compound) <-- intellig\n",
            "intellig (compound) <-- target\n",
            "target (compound) <-- marketkudo\n",
            "marketkudo (compound) <-- avatar\n",
            "cameron (compound) <-- avatar\n",
            "avatar (appos) <-- artific\n",
            "one (nummod) <-- year\n",
            "movi (compound) <-- year\n",
            "year (dobj) <-- give\n",
            "could (aux) <-- get\n",
            "get (conj) <-- begin\n",
            "troubl (compound) <-- share\n",
            "share (compound) <-- plea\n",
            "guy (nmod) <-- plea\n",
            "earli (amod) <-- plea\n",
            "plea (nsubj) <-- click\n",
            "click (ccomp) <-- get\n",
            "ye (compound) <-- use\n",
            "comment (compound) <-- use\n",
            "use (dobj) <-- click\n",
            "thank (dep) <-- begin\n",
            "Named Entity Counts:\n",
            "Person Names (PERSON): 2736\n",
            "Organizations (ORG): 1663\n",
            "Locations (GPE): 857\n",
            "Products (PRODUCT): 45\n",
            "Dates (DATE): 633\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spacy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the cleaned review CSV\n",
        "df = pd.read_csv('cleaned_imdb_reviews.csv')\n",
        "\n",
        "# Combine all cleaned reviews into one large text for analysis\n",
        "text = \" \".join(df['cleaned_review'].dropna().tolist())\n",
        "\n",
        "# Step 1: POS Tagging\n",
        "doc = nlp(text)\n",
        "pos_counts = Counter()\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']:  # Check for Nouns, Verbs, Adjectives, Adverbs\n",
        "        pos_counts[token.pos_] += 1\n",
        "\n",
        "print(\"POS Counts:\")\n",
        "print(f\"Nouns (N): {pos_counts['NOUN']}\")\n",
        "print(f\"Verbs (V): {pos_counts['VERB']}\")\n",
        "print(f\"Adjectives (Adj): {pos_counts['ADJ']}\")\n",
        "print(f\"Adverbs (Adv): {pos_counts['ADV']}\")\n",
        "\n",
        "# Step 2: Constituency and Dependency Parsing (using one sentence)\n",
        "doc_sample = nlp(df['cleaned_review'].dropna().tolist()[0])\n",
        "\n",
        "for sent in doc_sample.sents:\n",
        "    print(\"Sentence:\", sent.text)\n",
        "    print(\"Dependency Parsing:\")\n",
        "    for token in sent:\n",
        "        print(f\"{token.text} ({token.dep_}) <-- {token.head.text}\")\n",
        "    break  # Show parsing for the first sentence only\n",
        "\n",
        "# Step 3: Named Entity Recognition (NER)\n",
        "ner_counts = Counter()\n",
        "\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'DATE']:  # Filter out specific types of entities\n",
        "        ner_counts[ent.label_] += 1\n",
        "\n",
        "print(\"Named Entity Counts:\")\n",
        "print(\"Person Names (PERSON): {ner_counts['PERSON']}\")\n",
        "print(\"Organizations (ORG): {ner_counts['ORG']}\")\n",
        "print(\"Locations (GPE): {ner_counts['GPE']}\")\n",
        "print(\"Products (PRODUCT): {ner_counts['PRODUCT']}\")\n",
        "print(\"Dates (DATE): {ner_counts['DATE']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "This assignment took lot of time and at final I have completed it. As I am completely focused on all assignments and exercises.\n",
        "I have learned many things from this assignment. I faced only the part where the reviews I can extract but not all.\n",
        "So I tried to extract more reiews but it took me more time and finally from the html contents I undertood the way that\n",
        "we can able to get load more data from the page and then save the data in csv. This part of the exercise took me more time to complete the assignment."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}