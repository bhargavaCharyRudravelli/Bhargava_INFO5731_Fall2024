{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavaCharyRudravelli/Bhargava_INFO5731_Fall2024/blob/main/Rudravelli_Bhargava_03_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "fd37a8a5-e56a-4009-c4dc-529bc08c2dd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nTask: Email Spam Detection\\nDescription: Spam detection is the process of categorizing emails according to their content spam. This work assists in removing unsolicited emails and shielding users from possible dangers.\\n\\nFeatures for building the Machine Learning Model\\n\\n1. Bags of Words(BoW):\\n -->Provides a set of word frequencies to represent text.\\n -->It is useful to capture records terms like \"free,\" \"win,\" \"prize,\" and others that are frequently included in spam emails, as well as their frequency.\\n\\n2. Inverse Document Frequency-Term Frequency (TF-IDF):\\n -->It determines a word\\'s significance by comparing its frequency within a document to that of the full corpus.\\n -->It is useful in finding phrases that are more important in spam emails than in the total dataset, emphasizing distinct terms that might be signs of spam.\\n\\n3. Email Metadata:\\n\\n -->It features like the email address of the sender, the subject line, and the existence of specific keywords in the subject are included in this description.\\n -->The email metadata contains certain email addresses or subject line patterns will be linked to spam. Spam may be indicated, for instance, by emails with subjects like \"Congratulations\" or \"Urgent\" or from unknown domains.\\n\\n4. N-grams:\\n  -->Word sequences consisting of N letters, such as bigrams and trigrams.\\n  -->It captures word combinations and context that individual words could miss. For instance, the bigrams \"limited time\" and \"click here\" are often reliable signs of spam.\\n\\n5. HTML Features:\\n -->Presence of HTML tags and attributes in the email body.\\n -->Spam emails often contain HTML content with links, images, and formatting to make them look \\n    legitimate. The presence of certain HTML tags (e.g., <a>, <img>) can be indicative of spam.\\n\\n6. Special Characters and Punctuation:\\n -->Numbers of punctuation marks and special characters (like $, %, and @).\\n -->Special letters and a lot of punctuation are frequently used in spam emails to draw attention or hide content. Multiple dollar signs (\"$$$\") or exclamation points (\"!!!\"), for instance, could be indicators of spam.\\n\\n7. Word Embeddings:\\n -->Word representations in dense vectors (e.g., Word2Vec, GloVe).\\n -->This is helpful to captures the semantic connections between words, enabling the model to comprehend word similarity and context. For instance, the embeddings for \"offer\" and \"deal\" would be identical, suggesting spam.\\n\\n8. Text Length:\\n -->The email\\'s word count or character count.\\n -->This is helpful to spam emails typically have one of two lengths:\\n    very long with comprehensive offers, or very short with a call to action. \\n    Text length can aid in bringing other aspects into line.\\n\\nConclusion:\\nWe can create a powerful machine learning model for spam detection that incorporates a variety of email content and metadata by integrating these elements. \\nEmail filtering systems work better when emails are effectively classified as spam or not thanks to this multifaceted approach.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Task: Email Spam Detection\n",
        "Description: Spam detection is the process of categorizing emails according to their content spam. This work assists in removing unsolicited emails and shielding users from possible dangers.\n",
        "\n",
        "Features for building the Machine Learning Model\n",
        "\n",
        "1. Bags of Words(BoW):\n",
        " -->Provides a set of word frequencies to represent text.\n",
        " -->It is useful to capture records terms like \"free,\" \"win,\" \"prize,\" and others that are frequently included in spam emails, as well as their frequency.\n",
        "\n",
        "2. Inverse Document Frequency-Term Frequency (TF-IDF):\n",
        " -->It determines a word's significance by comparing its frequency within a document to that of the full corpus.\n",
        " -->It is useful in finding phrases that are more important in spam emails than in the total dataset, emphasizing distinct terms that might be signs of spam.\n",
        "\n",
        "3. Email Metadata:\n",
        "\n",
        " -->It features like the email address of the sender, the subject line, and the existence of specific keywords in the subject are included in this description.\n",
        " -->The email metadata contains certain email addresses or subject line patterns will be linked to spam. Spam may be indicated, for instance, by emails with subjects like \"Congratulations\" or \"Urgent\" or from unknown domains.\n",
        "\n",
        "4. N-grams:\n",
        "  -->Word sequences consisting of N letters, such as bigrams and trigrams.\n",
        "  -->It captures word combinations and context that individual words could miss. For instance, the bigrams \"limited time\" and \"click here\" are often reliable signs of spam.\n",
        "\n",
        "5. HTML Features:\n",
        " -->Presence of HTML tags and attributes in the email body.\n",
        " -->Spam emails often contain HTML content with links, images, and formatting to make them look\n",
        "    legitimate. The presence of certain HTML tags (e.g., <a>, <img>) can be indicative of spam.\n",
        "\n",
        "6. Special Characters and Punctuation:\n",
        " -->Numbers of punctuation marks and special characters (like $, %, and @).\n",
        " -->Special letters and a lot of punctuation are frequently used in spam emails to draw attention or hide content. Multiple dollar signs (\"$$$\") or exclamation points (\"!!!\"), for instance, could be indicators of spam.\n",
        "\n",
        "7. Word Embeddings:\n",
        " -->Word representations in dense vectors (e.g., Word2Vec, GloVe).\n",
        " -->This is helpful to captures the semantic connections between words, enabling the model to comprehend word similarity and context. For instance, the embeddings for \"offer\" and \"deal\" would be identical, suggesting spam.\n",
        "\n",
        "8. Text Length:\n",
        " -->The email's word count or character count.\n",
        " -->This is helpful to spam emails typically have one of two lengths:\n",
        "    very long with comprehensive offers, or very short with a call to action.\n",
        "    Text length can aid in bringing other aspects into line.\n",
        "\n",
        "Conclusion:\n",
        "We can create a powerful machine learning model for spam detection that incorporates a variety of email content and metadata by integrating these elements.\n",
        "Email filtering systems work better when emails are effectively classified as spam or not thanks to this multifaceted approach.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e577c031-0eb2-43d3-d76d-d042e8439342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   10  50  account  advisor  all  am  and  appointment  at  been  ...  PRP$  \\\n",
            "0   0   0        0        0    0   0    0            0   0     0  ...   1.0   \n",
            "1   0   0        1        0    0   0    0            0   0     1  ...   2.0   \n",
            "2   1   0        0        1    0   1    0            1   1     0  ...   1.0   \n",
            "3   0   1        0        0    1   0    0            0   0     0  ...   1.0   \n",
            "4   0   0        0        0    0   0    1            0   0     0  ...   0.0   \n",
            "\n",
            "     ,  VBZ  VBG   IN   CD   CC  WRB  POS   RP  \n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "1  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "2  0.0  1.0  1.0  2.0  1.0  0.0  0.0  0.0  0.0  \n",
            "3  0.0  0.0  0.0  3.0  1.0  0.0  0.0  0.0  0.0  \n",
            "4  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  \n",
            "\n",
            "[5 rows x 151 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text data\n",
        "emails = [\n",
        "    \"Congratulations! You've won a free prize. Click here to claim your reward.\",\n",
        "    \"Dear user, your account has been compromised. Please update your password immediately.\",\n",
        "    \"Meeting tomorrow at 10 AM. Your appointment with UNT Legal Advisor is scheduled.\",\n",
        "    \"Limited time offer! Get 50% off on all books in UNT library. Visit our website now.\",\n",
        "    \"Hi Bhargava, just wanted to check in and see how you're doing. Let's catch up soon.\"\n",
        "]\n",
        "\n",
        "# Initialize vectorizers\n",
        "count_vector = CountVectorizer()\n",
        "tfidf_vector = TfidfVectorizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "bow_features = count_vector.fit_transform(emails).toarray()\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_features = tfidf_vector.fit_transform(emails).toarray()\n",
        "\n",
        "# Email Metadata (for simplicity, using subject as part of the email text)\n",
        "def extract_metadata(email):\n",
        "    return {\n",
        "        'length': len(email),\n",
        "        'num_special_chars': sum(1 for char in email if char in \"!@#$%^&*()\"),\n",
        "        'num_html_tags': len(BeautifulSoup(email, \"html.parser\").find_all())\n",
        "    }\n",
        "\n",
        "metadata_features = [extract_metadata(email) for email in emails]\n",
        "\n",
        "# N-grams\n",
        "def extract_ngrams(email, n=2):\n",
        "    # Tokenize the email into words\n",
        "    words = word_tokenize(email)\n",
        "\n",
        "    # Create a list to store the n-grams\n",
        "    ngrams = []\n",
        "\n",
        "    # Loop through the list of words to create n-grams\n",
        "    num_words = len(words)\n",
        "    for start_index in range(num_words - n + 1):\n",
        "        # Extract n words starting from the current index\n",
        "        ngram_words = words[start_index:start_index + n]\n",
        "\n",
        "        # Join the n-gram words with a space to form a single string\n",
        "        ngram_string = \" \".join(ngram_words)\n",
        "\n",
        "        # Add the n-gram string to the list of n-grams\n",
        "        ngrams.append(ngram_string)\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "bigrams = [extract_ngrams(email, 2) for email in emails]\n",
        "\n",
        "# Part of Speech (POS) Tags\n",
        "def extract_pos_tags(email):\n",
        "    tokens = word_tokenize(email)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "    return pos_counts\n",
        "\n",
        "pos_tag_features = [extract_pos_tags(email) for email in emails]\n",
        "\n",
        "# Create DataFrames for the features\n",
        "bow_df = pd.DataFrame(bow_features, columns=count_vector.get_feature_names_out())\n",
        "tfidf_df = pd.DataFrame(tfidf_features, columns=[f\"tfidf_{word}\" for word in tfidf_vector.get_feature_names_out()])\n",
        "metadata_df = pd.DataFrame(metadata_features)\n",
        "pos_tag_df = pd.DataFrame(pos_tag_features).fillna(0)\n",
        "\n",
        "# Combine all features into a single DataFrame\n",
        "features_df = pd.concat([bow_df, tfidf_df, metadata_df, pos_tag_df], axis=1)\n",
        "\n",
        "print(features_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c3873e-6614-45ed-a2eb-d4ad89c0904d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        feature  chi2_score\n",
            "395    graphics  107.404265\n",
            "475       keith  106.852696\n",
            "385         god  102.261303\n",
            "655        pitt   93.206041\n",
            "373         geb   77.141680\n",
            "122       banks   74.653441\n",
            "390      gordon   74.486385\n",
            "582         msg   74.118948\n",
            "465       jesus   66.868713\n",
            "193      church   64.188361\n",
            "159     caltech   61.956683\n",
            "514     livesey   61.442825\n",
            "192  christians   59.816445\n",
            "189      christ   59.122114\n",
            "108     atheism   54.920004\n",
            "577    morality   54.437770\n",
            "354       files   54.228775\n",
            "35           3d   51.563985\n",
            "757     rutgers   49.022242\n",
            "437       image   48.457932\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load dataset\n",
        "categories_ds = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories_ds)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "tf_vector = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X = tf_vector.fit_transform(newsgroups_train.data)\n",
        "y = newsgroups_train.target\n",
        "\n",
        "# Apply Chi-Square feature selection\n",
        "chi2_scores, p_values = chi2(X, y)\n",
        "\n",
        "# Create a DataFrame with feature names and their Chi-Square scores\n",
        "feature_names = tf_vector.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'feature': feature_names, 'chi2_score': chi2_scores})\n",
        "\n",
        "# Rank features based on Chi-Square scores in descending order\n",
        "chi2_df = chi2_df.sort_values(by='chi2_score', ascending=False)\n",
        "\n",
        "# Select the top N features\n",
        "top_features = chi2_df.head(20)\n",
        "\n",
        "# Display the top features\n",
        "print(top_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1712e9-7c8f-4ba6-f46f-35fa86a13fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.6043, Email: Congratulations! You've won a free prize. Click here to claim your reward.\n",
            "Score: 0.5335, Email: Meeting tomorrow at 10 AM. Your appointment with UNT Legal Advisor is scheduled.\n",
            "Score: 0.5165, Email: Limited time offer! Get 50% off on all books in UNT library. Visit our website now.\n",
            "Score: 0.4297, Email: Hi Bhargava, just wanted to check in and see how you're doing. Let's catch up soon.\n",
            "Score: 0.4205, Email: Dear user, your account has been compromised. Please update your password immediately.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bt_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert(texts):\n",
        "    inputs = bt_tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Sample text data\n",
        "emails = [\n",
        "    \"Congratulations! You've won a free prize. Click here to claim your reward.\",\n",
        "    \"Dear user, your account has been compromised. Please update your password immediately.\",\n",
        "    \"Meeting tomorrow at 10 AM. Your appointment with UNT Legal Advisor is scheduled.\",\n",
        "    \"Limited time offer! Get 50% off on all books in UNT library. Visit our website now.\",\n",
        "    \"Hi Bhargava, just wanted to check in and see how you're doing. Let's catch up soon.\"\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"How to claim my prize?\"\n",
        "\n",
        "# Get BERT embeddings for emails and query\n",
        "email_embeddings = get_bert(emails)\n",
        "query_embedding = get_bert([query])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_scores = cosine_similarity(query_embedding, email_embeddings).flatten()\n",
        "\n",
        "# Rank emails based on similarity scores\n",
        "indices = np.argsort(similarity_scores)[::-1]\n",
        "emails = [emails[i] for i in indices]\n",
        "scores = [similarity_scores[i] for i in indices]\n",
        "\n",
        "# Print ranked emails and their similarity scores\n",
        "for email, score in zip(emails, scores):\n",
        "    print(f\"Score: {score:.4f}, Email: {email}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "This exercise taken much time to analyse and to understand the concepts. I gone through many websites and pyhton pages to understand these concepts.\n",
        "I understood the concepts of Machine learning the way it will handle the datasets that are Bag of Words, Term Frequency-Inverse Document Frequenct,\n",
        "N-grams and Parts of Speech.\n",
        "While writing and executing the code I have encountered many issues in the compile time and found compile errors.\n",
        "This is the most time taken to find out and resolve the issues.\n",
        "I have learned the techniques such as preprocessing, feature_extraction and many other which are the common concepts of Machine Learning.\n",
        "\n",
        "The study of NLP can greatly benefit from this practice. A crucial first in many natural language processing (NLP) applications, text categorization, machine translation, and information retrieval, is feature extraction from text input.\n",
        "I can efficiently preprocess and analyze textual material by comprehending and putting these strategies into practice.\n",
        "\n",
        "This exercise highlights the significance in the context of NLP such as Data Preprocessing and Feature Engineering\n",
        "This work has given me a strong basis in text feature extraction, which is essential for many jobs in natural language processing.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "87138a9e-8b33-46ab-bea1-d2b71c1bab46"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nThis exercise taken much time to analyse and to understand the concepts. I gone through many websites and pyhton pages to understand these concepts.\\nI understood the concepts of Machine learning the way it will handle the datasets that are Bag of Words, Term Frequency-Inverse Document Frequenct,\\nN-grams and Parts of Speech. \\nWhile writing and executing the code I have encountered many issues in the compile time and found compile errors.\\nThis is the most time taken to find out and resolve the issues. \\nI have learned the techniques such as preprocessing, feature_extraction and many other which are the common concepts of Machine Learning.\\n\\nThe study of NLP can greatly benefit from this practice. A crucial first in many natural language processing (NLP) applications, text categorization, machine translation, and information retrieval, is feature extraction from text input. \\nI can efficiently preprocess and analyze textual material by comprehending and putting these strategies into practice.\\n\\nThis exercise highlights the significance in the context of NLP such as Data Preprocessing and Feature Engineering \\nThis work has given me a strong basis in text feature extraction, which is essential for many jobs in natural language processing.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2PiIQmNMCmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}